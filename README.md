
# Stable Diffusion from Scratch ğŸ§ ğŸ¨

This is an educational implementation of **Stable Diffusion**, a latent text-to-image diffusion model, built from scratch using PyTorch. It demonstrates the end-to-end process of converting text prompts into high-resolution images using a denoising diffusion probabilistic model (DDPM), a CLIP text encoder, and a VAE.

---

## ğŸ§° Project Structure

```

.
â”œâ”€â”€ attention.py          # Self-attention modules used in the diffusion model
â”œâ”€â”€ clip.py               # CLIP-based text encoder
â”œâ”€â”€ ddpm.py               # DDPM sampler implementation
â”œâ”€â”€ decoder.py            # VAE decoder (not shown, assumed to be present)
â”œâ”€â”€ diffusion.py          # Core U-Net architecture for denoising
â”œâ”€â”€ encoder.py            # VAE encoder
â”œâ”€â”€ model\_loader.py       # Load and prepare pretrained model weights
â”œâ”€â”€ model\_converter.py    # Convert Hugging Face / other checkpoint formats
â”œâ”€â”€ pipeline.py           # Main generation pipeline
â”œâ”€â”€ demo.ipynb            # Jupyter notebook demo to run text-to-image generation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vocab.json        # Vocabulary for tokenizer
â”‚   â”œâ”€â”€ merges.txt        # Merge rules for BPE tokenizer
â”‚   â””â”€â”€ v1-5-pruned-emaonly.ckpt  # Pretrained Stable Diffusion checkpoint
â”œâ”€â”€ images/
â”‚   â””â”€â”€ dog.jpg           # Sample input image for image-to-image generation
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ output/           # Folder containing generated output images
â”‚   â””â”€â”€ output2/
â””â”€â”€ README.md             # You are here ğŸ“„

````

---

## ğŸš€ Features

- **Text-to-Image** generation using diffusion
- **Image-to-Image** transformation with noise control
- Uses **DDPM sampler**
- Classifier-Free Guidance support (CFG)
- Seed control for reproducibility
- CPU / CUDA / MPS support

---

## ğŸ“¦ Requirements

```bash
pip install torch torchvision
pip install transformers
pip install pillow tqdm
````

Also install `pytorch_lightning` (required for `.ckpt` checkpoint loading):

```bash
pip install pytorch_lightning
```

---

## ğŸ“ Usage

### Run from notebook

Use the Jupyter notebook to test both text-to-image and image-to-image generation:

```bash
jupyter notebook demo.ipynb
```

### Generate image from text prompt

In `demo.ipynb`, set:

```python
prompt = "A cat stretching on the floor, ultra sharp, 8k"
input_image = None
```

### Image-to-image transformation

Uncomment the following lines in the notebook:

```python
input_image = Image.open("../images/dog.jpg")
strength = 0.8  # Lower = closer to input
```

---

## ğŸ–¼ Sample Outputs

Outputs are saved in:

* `output/output/`
* `output/output2/`

Example images generated from prompts are stored here.

---

## ğŸ“Œ Notes

* This project is for **learning and experimentation**.
* Model weights and tokenizer files are expected in the `data/` folder.
* The implementation is simplified but faithful to the core Stable Diffusion pipeline.

---

## ğŸ“š References

* [Stable Diffusion Paper](https://arxiv.org/abs/2112.10752)
* [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)
* [Hugging Face Transformers](https://github.com/huggingface/transformers)

---

## ğŸ‘¤ Author

**Rohan Shenoy** â€” [GitHub Profile](https://github.com/rohanshenoy30)

```

Let me know if you'd like badges (e.g., Python version, license) or deployment instructions (e.g., Gradio/Web UI) added!
```
